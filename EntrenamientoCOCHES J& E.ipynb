{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"font-family:impact; font-size:2.5em;color:deepskyblue\">Entrenamiento Used Cars Dataset</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"font-family:impact; font-size:1.75em;color:royalblue\">Proyecto de Open Data II</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"font-family:impact; font-size:1em;color:mediumblue\">Elena Delgado y Javier García</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"font-family:monospace;color:coral\">ÍNDICE</span>\n",
    "1. Recordatorio de nuestro proyecto\n",
    "2. Preparación Datos\n",
    "3. SQL\n",
    "4. Learning method\n",
    "    + División de datos\n",
    "    - Pasos a seguir para crear predicciones\n",
    "    + Random Forest\n",
    "    - Gradient Boostes Tree Regressor\n",
    "    + Linear Regressor\n",
    "    - Grid Search con Random Forest\n",
    "    + Discretizar Variables Continuas\n",
    "    - Linear Regressor Disretized\n",
    "    + Estandarizar Variables Continuas\n",
    "    - Linear Regression Standarized\n",
    "    + PCA\n",
    "    - Comparación Final de Métodos\n",
    "5. Conclusiones "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd \n",
    "import numpy as np                         \n",
    "%matplotlib inline\n",
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()\n",
    "import pyspark\n",
    "import pyspark.sql.types as typ\n",
    "from pyspark.sql.types import *\n",
    "from pyspark.sql.session import SparkSession\n",
    "import pyspark.sql.functions as func\n",
    "from pyspark.sql.functions import col"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Welcome to\n",
      "      ____              __\n",
      "     / __/__  ___ _____/ /__\n",
      "    _\\ \\/ _ \\/ _ `/ __/  '_/\n",
      "   /__ / .__/\\_,_/_/ /_/\\_\\   version 2.4.4\n",
      "      /_/\n",
      "\n",
      "Using Python version 3.7.3 (default, Mar 27 2019 16:54:48)\n",
      "SparkSession available as 'spark'.\n"
     ]
    }
   ],
   "source": [
    "import os\n",
    "exec(open(os.path.join(os.environ[\"SPARK_HOME\"], 'python/pyspark/shell.py')).read())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"font-family:monospace;color:salmon\">RECORDATORIO DE NUESTRO PROYECTO</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "_Used Cars Dataset_ es un dataset que reune todas las ventas de vehículos de segunda mano dentro de Estados Unidos desde el año 1990 a 2019."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![.](https://lh5.googleusercontent.com/proxy/acYCTVi9zclt2LOeL2fNcXAHhSmLTgbMDq1ZeHlnUt9zXouj_f1MThWSgYvXIsvIp-4tWmgF9YoyuHYL3jG74FPHdaIlPGMa2KQlajF0pvrBxWgxe3Hej1IBS4GU8By0U8g1YBtAxhM)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Este dataset tiene las siguientes columnas:\n",
    "+ city\n",
    "+ price\n",
    "+ year\n",
    "+ manufacturer\n",
    "+ make\n",
    "+ condition\n",
    "+ cylinders\n",
    "+ fuel\n",
    "+ odometer\n",
    "+ transmission\n",
    "+ drive\n",
    "+ type\n",
    "+ paint_color"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"font-family:monospace;color:RED\">PREPARACIÓN DE DATOS</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"font-family:monospace;color:orangered\">Exportamos CSV</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Para comenzar, vamos a importar dos csv con spark.\n",
    "1. Coches_db es el csv final que obtuvimos en Proyecto de Open Data I donde todas las variables son numéricas excepto la columna de modelo de coche _make_. \n",
    "2. Coches1_db es el dataset que contiene variables categóricas y numéricas"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "coches_db = spark.read.csv('./coches.csv', header='true', inferSchema='true', sep=',') \n",
    "coches_db.createOrReplaceTempView(\"coche\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "coches1_db = spark.read.csv('./coches1.csv', header='true', inferSchema='true', sep=',') \n",
    "coches1_db.createOrReplaceTempView(\"coche\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a visualizar las columnas que tenia nuestro dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(_c0=0, city=1, price=9000, year=2009.0, manufacturer=2, make='suburban lt2', condition='1', cylinders=8, fuel=1, odometer=217743.0, transmission=1.0, drive=3, type=2, paint_color=2)]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coches_db.head(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"font-family:monospace;color:coral\">Creamos el esquema</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a coger todas las columnas de nuestro dataset para hacer el **esquema** que usaremos más tarde para entrenar nuestro modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "schema = StructType([\n",
    "    StructField(\"_c0\", IntegerType(), True),\n",
    "    StructField(\"city\", IntegerType(), True),\n",
    "    StructField(\"price\", IntegerType(), True),\n",
    "    StructField(\"year\", DoubleType(), True),\n",
    "    StructField(\"manufacturer\", IntegerType(), True),\n",
    "    StructField(\"make\", StringType(), True),\n",
    "    StructField(\"condition\", StringType(), True),\n",
    "    StructField(\"cylinders\", IntegerType(), True),\n",
    "    StructField(\"fuel\", IntegerType(), True),\n",
    "    StructField(\"odometer\", DoubleType(), True),\n",
    "    StructField(\"transmission\", DoubleType(), True),\n",
    "    StructField(\"drive\", IntegerType(), True),\n",
    "    StructField(\"type\", IntegerType(), True),\n",
    "    StructField(\"paint_color\", IntegerType(), True),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "coches_df = spark.read.csv('coches.csv', header='true', inferSchema='false', schema=schema, sep=',')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a visualizar el esquema para ver qué datos son necesarios y cuales no. Además nos va a servir para ver que se ha implementado correctamente."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "root\n",
      " |-- _c0: integer (nullable = true)\n",
      " |-- city: integer (nullable = true)\n",
      " |-- price: integer (nullable = true)\n",
      " |-- year: double (nullable = true)\n",
      " |-- manufacturer: integer (nullable = true)\n",
      " |-- make: string (nullable = true)\n",
      " |-- condition: string (nullable = true)\n",
      " |-- cylinders: integer (nullable = true)\n",
      " |-- fuel: integer (nullable = true)\n",
      " |-- odometer: double (nullable = true)\n",
      " |-- transmission: double (nullable = true)\n",
      " |-- drive: integer (nullable = true)\n",
      " |-- type: integer (nullable = true)\n",
      " |-- paint_color: integer (nullable = true)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "coches_df.printSchema()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a seleccionar columnas que queremos usar más tarde para la predicción y las metemos en **_cochesnuevo_**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "cochesnuevo=coches_df.select(\"price\",\"city\",\"year\",\"manufacturer\",\"cylinders\",\"fuel\",\"odometer\",\"transmission\",\"drive\",\"type\",\"paint_color\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "pyspark.sql.dataframe.DataFrame"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "type(cochesnuevo)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"font-family:monospace;color:orange\">Modificaciones de coches1_db</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a **eliminar** la columna todas las columnas numericas coches1_db ya que no nos van a ser utiles, y vamos a **renombrar** las columnas categóricas para que sea más sencillo de buscar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "columns_to_drop = ['_c0']\n",
    "coches1_db = coches1_db.drop(*columns_to_drop)\n",
    "columns_to_drop = ['price']\n",
    "coches1_db = coches1_db.drop(*columns_to_drop)\n",
    "columns_to_drop = ['year']\n",
    "coches1_db = coches1_db.drop(*columns_to_drop)\n",
    "columns_to_drop = ['cylinders']\n",
    "coches1_db = coches1_db.drop(*columns_to_drop)\n",
    "columns_to_drop = ['odometer']\n",
    "coches1_db = coches1_db.drop(*columns_to_drop)\n",
    "columns_to_drop = ['make']\n",
    "coches1_db = coches1_db.drop(*columns_to_drop)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "coches1_db=coches1_db.withColumnRenamed('city','state')\n",
    "coches1_db=coches1_db.withColumnRenamed('manufacturer','manufacturerN')\n",
    "coches1_db=coches1_db.withColumnRenamed('condition','conditionN')\n",
    "coches1_db=coches1_db.withColumnRenamed('fuel','fuelN')\n",
    "coches1_db=coches1_db.withColumnRenamed('transmission','transmissionN')\n",
    "coches1_db=coches1_db.withColumnRenamed('drive','driveN')\n",
    "coches1_db=coches1_db.withColumnRenamed('type','typeN')\n",
    "coches1_db=coches1_db.withColumnRenamed('paint_color','paint_colorN')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a visualizar las **columnas** que han quedado en el segundo Dataset para ver que las modificaciones que hemos implementado se han completado. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['state',\n",
       " 'manufacturerN',\n",
       " 'conditionN',\n",
       " 'fuelN',\n",
       " 'transmissionN',\n",
       " 'driveN',\n",
       " 'typeN',\n",
       " 'paint_colorN']"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "coches1_db.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"font-family:monospace;color:gold\">Visualizamos las tablas</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a visualizar las dos tablas para que nos facilite nuestro trabajo en caso de que necesitemos visualizar algún dato rapidamente o consultar el tipo de los datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+------------+---------+----+------------------+------------+-----+----+-----------+\n",
      "|price|city|  year|manufacturer|cylinders|fuel|          odometer|transmission|drive|type|paint_color|\n",
      "+-----+----+------+------------+---------+----+------------------+------------+-----+----+-----------+\n",
      "| 9000|   1|2009.0|           2|        8|   1|          217743.0|         1.0|    3|   2|          2|\n",
      "|31999|   1|2012.0|           6|        6|   2|111093.57002957871|         1.0|    1|   2|          3|\n",
      "|16990|   1|2003.0|           6|        6|   2| 153430.5303265941|         2.0|    1|   2|          1|\n",
      "| 6000|   1|2002.0|           8|        8|   1|          195000.0|         1.0|    1|   4|          2|\n",
      "|37000|   1|2012.0|           2|        8|   2|          178000.0|         1.0|    1|   4|          3|\n",
      "| 3700|   1|2003.0|           2|        8|   1|          269000.0|         1.0|    1|   4|          3|\n",
      "|19950|   1|2013.0|           1|        8|   1|          116792.0|         1.0|    1|   4|          2|\n",
      "|19999|   1|2006.0|           6|        6|   2| 145301.7774154097|         2.0|    1|   2|          3|\n",
      "|33950|   1|2015.0|           1|        8|   2|           77350.0|         1.0|    1|   4|          2|\n",
      "|25950|   1|2015.0|           1|        8|   1|          121030.0|         1.0|    1|   5|          2|\n",
      "+-----+----+------+------------+---------+----+------------------+------------+-----+----+-----------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "cochesnuevo.show(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+-------------+----------+------+-------------+------+------+------------+\n",
      "|state|manufacturerN|conditionN| fuelN|transmissionN|driveN| typeN|paint_colorN|\n",
      "+-----+-------------+----------+------+-------------+------+------+------------+\n",
      "|   TX|    chevrolet|      good|   gas|    automatic|   rwd|   SUV|       white|\n",
      "|   TX|          ram| excellent|diesel|    automatic|   4wd|   SUV|      silver|\n",
      "|   TX|          ram|      good|diesel|       manual|   4wd|   SUV|       black|\n",
      "|   TX|          gmc|      good|   gas|    automatic|   4wd|pickup|       white|\n",
      "|   TX|    chevrolet| excellent|diesel|    automatic|   4wd|pickup|      silver|\n",
      "|   TX|    chevrolet|      fair|   gas|    automatic|   4wd|pickup|      silver|\n",
      "|   TX|         ford| excellent|   gas|    automatic|   4wd|pickup|       white|\n",
      "|   TX|          ram| excellent|diesel|       manual|   4wd|   SUV|      silver|\n",
      "|   TX|         ford| excellent|diesel|    automatic|   4wd|pickup|       white|\n",
      "|   TX|         ford| excellent|   gas|    automatic|   4wd| truck|       white|\n",
      "+-----+-------------+----------+------+-------------+------+------+------------+\n",
      "only showing top 10 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "coches1_db.show(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"font-family:monospace;color:chartreuse\">SQL</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![.](https://lh3.googleusercontent.com/proxy/Pl8zlhBktOGY9YLi2u67L_OGp33oQx0ToxJZdZSKLFAX1blDBScEbNZbp7u1pgC0tkXdsBy4n7YWEmdeDxh0O_Awa3rp5XHIbG2bgtwFMaawEAeiZ5qLiN3lUq2ap_SwSq_mtnMUOYgu6d_FrC1aX4qMpyoMqQ)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a crear tablas *SQL* para poder visualizar nuestros datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "cochesnuevo.createOrReplaceTempView(\"cochesnuevo_tb\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "coches1_db.createOrReplaceTempView(\"coches1_tb\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a visualizar cuantas filas tenemos en ambos datasets, para saber que no se ha modificado nada anteriormente y ver con cuantos datos trabajamos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(count(1)=417197)]"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select count(*) from cochesnuevo_tb\").collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(count(1)=417197)]"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql(\"select count(*) from coches1_tb\").collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a visualizar los colores de coches disintos que tenemos al igual que los diferentes estados de nuestros datos, para que ver que todo sigue bien."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(paint_colorN='orange'),\n",
       " Row(paint_colorN='grey'),\n",
       " Row(paint_colorN='green'),\n",
       " Row(paint_colorN='yellow'),\n",
       " Row(paint_colorN='silver'),\n",
       " Row(paint_colorN='purple'),\n",
       " Row(paint_colorN='white'),\n",
       " Row(paint_colorN='red'),\n",
       " Row(paint_colorN='custom'),\n",
       " Row(paint_colorN='black'),\n",
       " Row(paint_colorN='brown'),\n",
       " Row(paint_colorN='truck'),\n",
       " Row(paint_colorN='blue')]"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "spark.sql('select distinct(paint_colorN) from coches1_tb').collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+\n",
      "|state|\n",
      "+-----+\n",
      "|   SC|\n",
      "|   AZ|\n",
      "|   LA|\n",
      "|   MN|\n",
      "|   NJ|\n",
      "|   DC|\n",
      "|   OR|\n",
      "|   VA|\n",
      "|   RI|\n",
      "|   KY|\n",
      "|   WY|\n",
      "|   NH|\n",
      "|   MI|\n",
      "|   NV|\n",
      "|   WI|\n",
      "|   ID|\n",
      "|   CA|\n",
      "|   CT|\n",
      "|   NE|\n",
      "|   MT|\n",
      "+-----+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "coches1_db.select('state').distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Una vez tenemos todos los Datasets revisados, vamos a entrenar nuestro modelo. Como nuestro Dataset no fue sacado de los challenges y nuestro proposito desde que empezamos a manejar los datos fue predecir el precio futuro de un coche, ahora vamos a estudiar con métodos de predicción adecuados los precios."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"font-family:monospace;color:turquoise\">LEARNING METHOD</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![.](https://images.squarespace-cdn.com/content/5be4713f70e80254dd35b8b3/1549656674295-0WPM8V4DZ97XGPR01S13/Machine-learning-BG.jpeg?format=1500w&content-type=image%2Fjpeg)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.ml.feature as ft\n",
    "import pyspark.ml.regression as rg\n",
    "from pyspark.ml import Pipeline\n",
    "import pyspark.ml.evaluation as ev\n",
    "from pyspark.ml.feature import VectorAssembler, VectorIndexer\n",
    "from pyspark.ml.evaluation import RegressionEvaluator\n",
    "from pyspark.ml.param import Param, Params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Un modelo de regresión es un modelo matemático que busca determinar la relación entre una variable dependiente (Y) con respecto a otras variables llamadas explicativas o independientes (X). Asimismo, el modelo busca determinar cuál será el impacto sobre la variable Y ante un cambio en las variables explicativas (X)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Puesto que nosotros tenemos que hacer una regresión, vamos a ver que métodos de entrenamiento teneos. \n",
    "+ Linear regression\n",
    "+ Decision tree regression\n",
    "+ Random forest regression\n",
    "+ Gradient-boosted tree regression\n",
    "+ Isotonic regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"font-family:monospace;color:deepskyblue\">División de datos</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a dividir nuestros datos, 80 % va a ser de los datos de entrenamiento y el otro 20 % es para los datos de test. Ambos que luego se usaran para entrenar el modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the data into training and test sets (20% held out for testing)\n",
    "(trainingData, testData) = cochesnuevo.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Antes de comenzar, vamos a pasar nuestra columna de price para que sea de DoubleType y así podamos usarla en los modelos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "cochesnuevo = cochesnuevo.withColumn('price', cochesnuevo['price'].cast(typ.DoubleType()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"font-family:monospace;color:royalblue\">Pasos a seguir para crear predicciones con nuestros algoritmos</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Para realizar este algoritmo vamos a empezar creando una columna que reúna todas las variables utilizando los **transformadores**.\n",
    "2. Preparar nuestro **modelo** con sus respectivos valores en caso de tener.\n",
    "3. Creamos el **pipeline**. El Pipeline nos va a ayudar a meter nuestros transformadores y el modelo para ejecutarlo uno detrás de otro.\n",
    "4. **Modelamos** nuestros datos de training el cual tiene el 80% de nuestros datos.\n",
    "5. A continuación **testeamos** el modelo \n",
    "6. **Visualizamos** las predicciones que hemos obtenido.\n",
    "7. **Evaluamos** el modelo \n",
    "8. Calculamos el **RMSE** (raíz del error cuadrático medio) y el **MSE** (error cuadrático medio)  de nuestro modelo. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"font-family:monospace;color:mediumslateblue\">RANDOM FOREST</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import RandomForestRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**¿Es *Random Forest* un algoritmo apropiado para nuestros datos?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![.](https://www.paradigmadigital.com/wp-content/uploads/2017/03/machine-learning-dummies-6.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "El primer algoritmo que decidimos utilizar fue Random Forest ya que este algoritmo de regresión consiste en un conjunto de múltiples árboles de decisión independientes, al cual se le asigna un conjunto de datos aleatorios con una misma distribución. La salida es el promedio de los resultados finales de cada árbol."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a empezar creando una columna que reúna todas las variables utilizando el transformador **VectorAssembler**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "mivectorRF = ft.VectorAssembler(\n",
    "    inputCols=[\"city\",\"year\",\"manufacturer\",\"cylinders\",\"fuel\",\"odometer\",\"transmission\",\"drive\",\"type\",\"paint_color\"], \n",
    "    outputCol='features'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "+ Ahora vamos a preparar nuestro modelo con sus valores:\n",
    "    + 5 árboles para entrenar nuestros datos\n",
    "    + 5 niveles de profundidad de los árboles\n",
    "    + 'price', la columna que tiene que predecir"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "algoritmoRF = rg.RandomForestRegressor(\n",
    "    numTrees=5, \n",
    "    maxDepth=5, \n",
    "    labelCol='price')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Utilizamos el **Pipeline** ya que nos va a ayudar a meter nuestro VectorAssembler y el modelo para ejecutarlo uno detrás de otro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelineRF = Pipeline(stages=[mivectorRF, algoritmoRF])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modelamos** nuestros datos de **training** el cual tiene el 80% de nuestros datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelRF = pipelineRF.fit(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a poner al modelo a que haga las **prediciones** con los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "testRF = modelRF.transform(testData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualizamos** las dos primeras columnas para ver que prediccion nos ha hecho nuestro modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(price=2050, city=5, year=1999.0, manufacturer=3, cylinders=6, fuel=1, odometer=202000.0, transmission=1.0, drive=2, type=1, paint_color=2, features=DenseVector([5.0, 1999.0, 3.0, 6.0, 1.0, 202000.0, 1.0, 2.0, 1.0, 2.0]), prediction=5173.5504671237995),\n",
       " Row(price=2100, city=2, year=1996.0, manufacturer=1, cylinders=6, fuel=1, odometer=151235.29235237173, transmission=1.0, drive=1, type=1, paint_color=2, features=DenseVector([2.0, 1996.0, 1.0, 6.0, 1.0, 151235.2924, 1.0, 1.0, 1.0, 2.0]), prediction=5173.5504671237995)]"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testRF.take(2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ahora vamos a **evaluar** nuestro modelo para ver la raíz del error cuadratico medio ***RMSE***, y el error cuadrático medio ***MSE***. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 10753.2\n"
     ]
    }
   ],
   "source": [
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"price\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmseRF = evaluator.evaluate(testRF)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmseRF) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE) on test data = 1.15632e+08\n"
     ]
    }
   ],
   "source": [
    "mseRF = rmseRF*rmseRF\n",
    "print(\"Mean Squared Error (MSE) on test data = %g\" % mseRF) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Contestando a la pregunta incial, podemos ver que Random Forest tiene un alto error cuadrático medio para nuestro modelo, por lo que **NO** ha realizado una buena predicción."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"font-family:monospace;color:darkorchid\">GRADIENT BOOSTED TREE REGRESSOR</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import GBTRegressor"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**¿Es _Gradient Boosted Tree Regressor_ un algoritmo apropiado para nuestros datos?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![.](https://littleml.files.wordpress.com/2017/03/boosted-trees-process.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Gradient Boosted Tree Regressor es una técnica de aprendizaje automático que produce un modelo de predicción en forma de un conjunto de modelos de predicción débiles, típicamente árboles de decisión. En estos casos, construye cada árbol de regresión de forma gradual, utilizando una función de pérdida predefinida para medir el error en cada paso y corregirlo en el siguiente."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Habiendo visto que Random Forest no es un modelo adecuado para nuestros datos, hemos decidido comprobar si este modelo nos proporciona mejores predicciones. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al igual que con el método anterior, vamos a empezar creando una columna que reúna todas las variables utilizando **transformadores**."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresCols = cochesnuevo.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "mivectorGBT = VectorAssembler(inputCols=featuresCols, outputCol=\"rawFeatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This identifies categorical features and indexes them.\n",
    "vectorIndexer = VectorIndexer(inputCol=\"rawFeatures\", outputCol=\"features\", maxCategories=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Preparamos nuestro **modelo** con columna a predecir 'price'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "gbt = GBTRegressor(labelCol=\"price\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos el **pipeline**  para ejecutar nuestro VectorAssembler, el VectorIndexer y el modelo uno detrás de otro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelineGBT = Pipeline(stages=[mivectorGBT,vectorIndexer, gbt])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizamos el **modelado** de nuestros datos de training (80% de los datos). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelGBT = pipelineGBT.fit(trainingData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Entrenamos** el modelo con los datos de test (20%)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "testGBT = modelGBT.transform(testData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualizamos** las tres primeras prediciones para ver como ha funcionado nuestro modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(price=2050, city=5, year=1999.0, manufacturer=3, cylinders=6, fuel=1, odometer=202000.0, transmission=1.0, drive=2, type=1, paint_color=2, rawFeatures=DenseVector([2050.0, 5.0, 1999.0, 3.0, 6.0, 1.0, 202000.0, 1.0, 2.0, 1.0, 2.0]), features=DenseVector([2050.0, 5.0, 1999.0, 3.0, 6.0, 1.0, 202000.0, 1.0, 1.0, 1.0, 2.0]), prediction=2429.1004923032024),\n",
       " Row(price=2100, city=2, year=1996.0, manufacturer=1, cylinders=6, fuel=1, odometer=151235.29235237173, transmission=1.0, drive=1, type=1, paint_color=2, rawFeatures=DenseVector([2100.0, 2.0, 1996.0, 1.0, 6.0, 1.0, 151235.2924, 1.0, 1.0, 1.0, 2.0]), features=DenseVector([2100.0, 2.0, 1996.0, 1.0, 6.0, 1.0, 151235.2924, 1.0, 0.0, 1.0, 2.0]), prediction=2429.1004923032024),\n",
       " Row(price=2100, city=2, year=1999.0, manufacturer=5, cylinders=4, fuel=1, odometer=162695.9298187809, transmission=1.0, drive=1, type=6, paint_color=1, rawFeatures=DenseVector([2100.0, 2.0, 1999.0, 5.0, 4.0, 1.0, 162695.9298, 1.0, 1.0, 6.0, 1.0]), features=DenseVector([2100.0, 2.0, 1999.0, 5.0, 4.0, 1.0, 162695.9298, 1.0, 0.0, 6.0, 1.0]), prediction=2426.0747409989212)]"
      ]
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testGBT.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluamos** la precisión de predicción de nuestro modelo con un _RegressionEvaluator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"price\", predictionCol=\"prediction\", metricName=\"rmse\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculamos el **RMSE** (raíz del error cuadrático medio) y el **MSE** (error cuadrático medio)  de nuestro modelo. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 6694.64\n"
     ]
    }
   ],
   "source": [
    "rmseGBT= evaluator.evaluate(testGBT)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmseGBT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE) on test data = 4.48182e+07\n"
     ]
    }
   ],
   "source": [
    "mseGBT = rmseGBT*rmseGBT\n",
    "print(\"Mean Squared Error (MSE) on test data = %g\" % mseGBT) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos observar que el modelo de GRADIENT BOOSTED TREE REGRESOR nos da una predicción mucho mejor que RANDOM FOREST, pero nos continua dando un error cuadratico medio alto. Con esto podemos decir que **NO** es un buen método para nuestros datos "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"font-family:monospace;color:orchid\">LINEAR REGRESSION</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**¿Es _Linear Regression_ un algoritmo apropiado para nuestros datos?**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![.](https://media.geeksforgeeks.org/wp-content/uploads/python-linear-regression-4.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Linear regression es una técnica estadística donde podemos identificar que variables independientes (causas) explican una variable dependiente (resultado)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a comenzar creando una columna que reúna todas las variables utilizando el transformador **VectorAssembler**. A esta columna la llamaremos _'rawFeatures'_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "mivectorLR = VectorAssembler(inputCols=featuresCols, outputCol=\"rawFeatures\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# This identifies categorical features and indexes them.\n",
    "vectorIndexerLR = VectorIndexer(inputCol=\"rawFeatures\", outputCol=\"features\", maxCategories=4)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Montamos el **modelo** de Linear Regresision para predecir la columna 'price'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Takes the \"features\" column and learns to predict \"price_cluster\"\n",
    "lr = LinearRegression(labelCol=\"price\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al igual que los casos anteriores, creamos el **pipeline**  para ejecutar nuestro VectorAssembler, el VectorIndexer y el modelo uno detrás de otro."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipelineLR = Pipeline(stages=[mivectorLR, vectorIndexerLR, lr])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizamos el **modelado** del 80% de los datos y **entrenamos** el modelo con el 20% restante"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelLR = pipelineLR.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "testLR = modelLR.transform(testData)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Visualizamos** las tres primeras prediciones para ver como ha funcionado nuestro modelo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(price=2050, city=5, year=1999.0, manufacturer=3, cylinders=6, fuel=1, odometer=202000.0, transmission=1.0, drive=2, type=1, paint_color=2, rawFeatures=DenseVector([2050.0, 5.0, 1999.0, 3.0, 6.0, 1.0, 202000.0, 1.0, 2.0, 1.0, 2.0]), features=DenseVector([2050.0, 5.0, 1999.0, 3.0, 6.0, 1.0, 202000.0, 1.0, 1.0, 1.0, 2.0]), prediction=2050.0000000006953),\n",
       " Row(price=2100, city=2, year=1996.0, manufacturer=1, cylinders=6, fuel=1, odometer=151235.29235237173, transmission=1.0, drive=1, type=1, paint_color=2, rawFeatures=DenseVector([2100.0, 2.0, 1996.0, 1.0, 6.0, 1.0, 151235.2924, 1.0, 1.0, 1.0, 2.0]), features=DenseVector([2100.0, 2.0, 1996.0, 1.0, 6.0, 1.0, 151235.2924, 1.0, 0.0, 1.0, 2.0]), prediction=2100.000000000953),\n",
       " Row(price=2100, city=2, year=1999.0, manufacturer=5, cylinders=4, fuel=1, odometer=162695.9298187809, transmission=1.0, drive=1, type=6, paint_color=1, rawFeatures=DenseVector([2100.0, 2.0, 1999.0, 5.0, 4.0, 1.0, 162695.9298, 1.0, 1.0, 6.0, 1.0]), features=DenseVector([2100.0, 2.0, 1999.0, 5.0, 4.0, 1.0, 162695.9298, 1.0, 0.0, 6.0, 1.0]), prediction=2100.000000000861)]"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testLR.take(3)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Evaluamos** la precisión de predicción de nuestro modelo con un _RegressionEvaluator_. Al ver que  las predicciones de _linear regression_ son bastante parecidas a las originales podemos suponer que el error cuadrático medio será muy bajo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 3.88653e-10\n"
     ]
    }
   ],
   "source": [
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"price\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmseLR = evaluator.evaluate(testLR)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmseLR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calculamos el **MSE** (error cuadrático medio) de nuestro modelo."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE) on test data = 1.51051e-19\n"
     ]
    }
   ],
   "source": [
    "mseLR = rmseLR*rmseLR\n",
    "print(\"Mean Squared Error (MSE) on test data = %g\" % mseLR) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Viendo que el error cuadrático medio es prácticamente nulo, podemos confirmar que Linear Regression **SI** es un buen modelo para predecir nuestros datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"font-family:monospace;color:deeppink\">GRID SEARCH RANDOM FOREST</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a hacer un grid search de random forest para ver que parametros son los mejores para la prediccion de datos.Grid search es el proceso de escanear los datos para configurar parámetros óptimos para un modelo dado. Grid-Search creará un modelo en cada combinación de parámetros posible. Recorre cada combinación de parámetros y almacena un modelo para cada combinación."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark.ml.tuning as tune\n",
    "\n",
    "forest =  rg.RandomForestRegressor(labelCol='price')\n",
    "\n",
    "grid = tune.ParamGridBuilder().addGrid(forest.numTrees, [2, 5, 10, 15]).addGrid(forest.maxDepth, [5, 3, 2, 7]).build()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "evaluator =RegressionEvaluator(predictionCol='prediction', labelCol='price')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Creamos la lógica que realizará la **validación**. El modelo itera através del grid de valores, estima los modelos y comparar su rendimiento utilizando el \"evaluator\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgC = tune.CrossValidator(estimator=forest, estimatorParamMaps=grid, evaluator=evaluator,numFolds=2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[mivectorRF])\n",
    "data_transformer = pipeline.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_transformer = pipeline.fit(trainingData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "rgModel = rgC.fit(data_transformer.transform(trainingData))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_train = data_transformer.transform(testData)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = rgModel.transform(data_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(price=2050, city=5, year=1999.0, manufacturer=3, cylinders=6, fuel=1, odometer=202000.0, transmission=1.0, drive=2, type=1, paint_color=2, features=DenseVector([5.0, 1999.0, 3.0, 6.0, 1.0, 202000.0, 1.0, 2.0, 1.0, 2.0]), prediction=4040.8714597215762)]"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results.take(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 10122.1\n"
     ]
    }
   ],
   "source": [
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"price\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmseGS = evaluator.evaluate(results)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmseGS)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE) on test data = 1.02457e+08\n"
     ]
    }
   ],
   "source": [
    "mseGS = rmseGS*rmseGS\n",
    "print(\"Mean Squared Error (MSE) on test data = %g\" % mseGS) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos observar que, aunque tenga los parametros más adecuados, el error de predicción sigue siendo muy alto, por lo que esto nos confirma que Random Forest **NO** es el método adecuado para nuestros datos"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "([{'numTrees': 2}, {'maxDepth': 2}], 10020.538996650168)"
      ]
     },
     "execution_count": 67,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results = [\n",
    "    (\n",
    "        [\n",
    "            {key.name: paramValue} \n",
    "            for key, paramValue \n",
    "            in zip(\n",
    "                params.keys(), \n",
    "                params.values())\n",
    "        ], metric\n",
    "    ) \n",
    "    for params, metric \n",
    "    in zip(\n",
    "        rgModel.getEstimatorParamMaps(), \n",
    "        rgModel.avgMetrics\n",
    "    )\n",
    "]\n",
    "\n",
    "sorted(results, key=lambda el: el[1], reverse=True)[0]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![.](https://civilian.com/wp-content/uploads/2020/04/Campaign_Measurement.png)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"font-family:monospace;color:crimson\">DISCRETIZAR VARIABLES CONTINUAS </span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Discretizar datos quiere decir convertir variables que son continuas en variables agrupadas por intervalos. Esta operación simplifica la información agrupando los objetos geográficos que presentan las mismas características en distintas clases. Hemos discretizado la columna de Price, ya que esta al ser una medición es una variable contínua."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = np.arange(0, 100)\n",
    "x = x / 100.0 * np.pi * 4\n",
    "y = x * np.sin(x / 1.764) + 20.1234\n",
    "\n",
    "schema = typ.StructType([\n",
    "    typ.StructField('price', \n",
    "                    typ.DoubleType(), \n",
    "                    False\n",
    "   )\n",
    "])\n",
    "\n",
    "data = sqlContext.createDataFrame([[float(e), ] for e in y], schema=schema)\n",
    "#data = spark.createDataFrame([[float(e), ] for e in y], schema=schema)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Vamos a utilizar el _QuantileDiscretizer_ para partir nuestra variable continua en 5 buckets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "discretizer = ft.QuantileDiscretizer(\n",
    "    numBuckets=5, \n",
    "    inputCol='price', \n",
    "    outputCol='discretized')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(discretized=0.0, avg(price)=3715.7660397764826),\n",
       " Row(discretized=1.0, avg(price)=6998.078737565828),\n",
       " Row(discretized=2.0, avg(price)=11546.692363325943),\n",
       " Row(discretized=3.0, avg(price)=17883.877535947013),\n",
       " Row(discretized=4.0, avg(price)=32990.87914754255)]"
      ]
     },
     "execution_count": 70,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_discretized = discretizer.fit(cochesnuevo).transform(cochesnuevo)\n",
    "\n",
    "data_discretized \\\n",
    "    .groupby('discretized')\\\n",
    "    .mean('price')\\\n",
    "    .sort('discretized')\\\n",
    "    .collect()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"font-family:monospace;color:red\">LINEAR REGRESSION DISCRETIZED</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Como *Regresión Lineal* es el algoritmo que mejor predicción nos ha dado, vamos a utilizarlo con los datos discretizados que acabamos de hallar."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al tener nuevos datos, debemos volver a separar nuestros datos en entrenamiento y test en un 80-20"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainingDataD, testDataD) = data_discretized.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizamos los MISMOS PASOS que Linear Regressionn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelLRD = pipelineLR.fit(trainingDataD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {},
   "outputs": [],
   "source": [
    "testLRD = modelLRD.transform(testDataD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(price=2003.0, city=6, year=2003.0, manufacturer=4, cylinders=6, fuel=1, odometer=120000.0, transmission=1.0, drive=2, type=1, paint_color=2, discretized=0.0, rawFeatures=DenseVector([2003.0, 6.0, 2003.0, 4.0, 6.0, 1.0, 120000.0, 1.0, 2.0, 1.0, 2.0]), features=DenseVector([2003.0, 6.0, 2003.0, 4.0, 6.0, 1.0, 120000.0, 1.0, 1.0, 1.0, 2.0]), prediction=2002.9999999995505),\n",
       " Row(price=2013.0, city=6, year=2013.0, manufacturer=14, cylinders=4, fuel=1, odometer=98895.41200787401, transmission=2.0, drive=2, type=1, paint_color=2, discretized=0.0, rawFeatures=DenseVector([2013.0, 6.0, 2013.0, 14.0, 4.0, 1.0, 98895.412, 2.0, 2.0, 1.0, 2.0]), features=DenseVector([2013.0, 6.0, 2013.0, 14.0, 4.0, 1.0, 98895.412, 2.0, 1.0, 1.0, 2.0]), prediction=2013.0000000002572),\n",
       " Row(price=2075.0, city=32, year=2003.0, manufacturer=5, cylinders=6, fuel=1, odometer=153430.5303265941, transmission=1.0, drive=1, type=1, paint_color=2, discretized=0.0, rawFeatures=DenseVector([2075.0, 32.0, 2003.0, 5.0, 6.0, 1.0, 153430.5303, 1.0, 1.0, 1.0, 2.0]), features=DenseVector([2075.0, 32.0, 2003.0, 5.0, 6.0, 1.0, 153430.5303, 1.0, 0.0, 1.0, 2.0]), prediction=2074.9999999996016)]"
      ]
     },
     "execution_count": 74,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testLRD.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 75,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 6.72951e-10\n"
     ]
    }
   ],
   "source": [
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"price\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmseLRD = evaluator.evaluate(testLRD)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmseLRD)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 76,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE) on test data = 4.52862e-19\n"
     ]
    }
   ],
   "source": [
    "mseLRD = rmseLRD*rmseLRD\n",
    "print(\"Mean Squared Error (MSE) on test data = %g\" % mseLRD) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos observar, que con datos discretizados, el error de predicción con _Logistic Regression_ sigue siendo una buena opción para nuestros datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"font-family:monospace;color:tomato\">ESTANDARIZAR VARIABLES CONTINUAS</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "La estandarización o normalización de índices significa ajustar los valores medidos en diferentes escalas respecto a una escala común, a menudo previo a un proceso de realizar promedios."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = ft.VectorAssembler(inputCols=['price'], outputCol= 'price_vec')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Construimos el \"normalizer\" y el \"pipeline\". Pondremos \"withMean=True\" y \"withStd=True\" para que la media y la varianza sean de un longitud de uno."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {},
   "outputs": [],
   "source": [
    "normalizer = ft.StandardScaler(\n",
    "    inputCol=vectorizer.getOutputCol(), \n",
    "    outputCol='normalized', \n",
    "    withMean=True,\n",
    "    withStd=True\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [],
   "source": [
    "pipeline = Pipeline(stages=[vectorizer, normalizer])\n",
    "data_standardized = pipeline.fit(cochesnuevo).transform(cochesnuevo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 80,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(price=9000.0, city=1, year=2009.0, manufacturer=2, cylinders=8, fuel=1, odometer=217743.0, transmission=1.0, drive=3, type=2, paint_color=2, price_vec=DenseVector([9000.0]), normalized=DenseVector([-0.4383]))]"
      ]
     },
     "execution_count": 80,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data_standardized.take(1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"font-family:monospace;color:lightsalmon\">LINEAR REGRESSION STANDARIZED</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Al igual que en el caso anterior, vamos a utilizar *Regresión Lineal* ya que es el algoritmo que mejor predicción nos ha dado, por lo vamos a utilizarlo con los datos estandarizados que acabamos de hallar."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "(trainingDataE, testDataE) = data_discretized.randomSplit([0.8, 0.2])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Realizamos los MISMOS PASOS que Linear Regressionn"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "modelLRE = pipelineLR.fit(trainingDataE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [],
   "source": [
    "testLRE = modelLRE.transform(testDataE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[Row(price=2003.0, city=6, year=2003.0, manufacturer=4, cylinders=6, fuel=1, odometer=120000.0, transmission=1.0, drive=2, type=1, paint_color=2, discretized=0.0, rawFeatures=DenseVector([2003.0, 6.0, 2003.0, 4.0, 6.0, 1.0, 120000.0, 1.0, 2.0, 1.0, 2.0]), features=DenseVector([2003.0, 6.0, 2003.0, 4.0, 6.0, 1.0, 120000.0, 1.0, 1.0, 1.0, 2.0]), prediction=2002.999999999999),\n",
       " Row(price=2050.0, city=7, year=2003.0, manufacturer=6, cylinders=8, fuel=1, odometer=175500.0, transmission=1.0, drive=3, type=4, paint_color=9, discretized=0.0, rawFeatures=DenseVector([2050.0, 7.0, 2003.0, 6.0, 8.0, 1.0, 175500.0, 1.0, 3.0, 4.0, 9.0]), features=DenseVector([2050.0, 7.0, 2003.0, 6.0, 8.0, 1.0, 175500.0, 1.0, 2.0, 4.0, 9.0]), prediction=2050.000000000008),\n",
       " Row(price=2050.0, city=23, year=2000.0, manufacturer=17, cylinders=6, fuel=1, odometer=156888.25572112825, transmission=1.0, drive=1, type=1, paint_color=2, discretized=0.0, rawFeatures=DenseVector([2050.0, 23.0, 2000.0, 17.0, 6.0, 1.0, 156888.2557, 1.0, 1.0, 1.0, 2.0]), features=DenseVector([2050.0, 23.0, 2000.0, 17.0, 6.0, 1.0, 156888.2557, 1.0, 0.0, 1.0, 2.0]), prediction=2049.9999999999986)]"
      ]
     },
     "execution_count": 89,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "testLRE.take(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Root Mean Squared Error (RMSE) on test data = 5.20874e-12\n"
     ]
    }
   ],
   "source": [
    "# Select (prediction, true label) and compute test error\n",
    "evaluator = RegressionEvaluator(\n",
    "    labelCol=\"price\", predictionCol=\"prediction\", metricName=\"rmse\")\n",
    "rmseLRE = evaluator.evaluate(testLRE)\n",
    "print(\"Root Mean Squared Error (RMSE) on test data = %g\" % rmseLRE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean Squared Error (MSE) on test data = 2.7131e-23\n"
     ]
    }
   ],
   "source": [
    "mseLRE = rmseLRE*rmseLRE\n",
    "print(\"Mean Squared Error (MSE) on test data = %g\" % mseLRE) "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos observar que con datos estandarizados el error de predicción sigue siendo casi nulo, por lo que este método es muy útil para nuestros datos"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"font-family:monospace;color:seagreen\">PCA</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![.](https://i2.wp.com/www.sportscidata.com/wp-content/uploads/2019/08/Principal_Component_Analysis_print.png?fit=1024%2C683&ssl=1)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "PCA es una técnica para resaltar patrones fuertes de un conjunto de datos. Este método se utiliza a menudo para facilitar la visualización y exploración de los datos."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "from pyspark.ml.feature import ChiSqSelector"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [],
   "source": [
    "featuresCreatorPCA = ft.VectorAssembler(\n",
    "    inputCols=[\"city\",\"year\",\"manufacturer\",\"cylinders\",\"fuel\",\"odometer\",\"transmission\",\"drive\",\"type\",\"paint_color\"], \n",
    "    outputCol='features'\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.ml.feature import PCA\n",
    "pca = PCA(k=2, inputCol='features', outputCol='pcaFeature')\n",
    "algoritmoPCA = rg.LinearRegression(labelCol=\"price\")\n",
    "pipeline = Pipeline(stages=[featuresCreatorPCA,pca,algoritmoPCA]) #pca"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+-----+----+------+------------+---------+----+------------------+------------+-----+----+-----------+--------------------+--------------------+-------------------+\n",
      "|price|city|  year|manufacturer|cylinders|fuel|          odometer|transmission|drive|type|paint_color|            features|          pcaFeature|         prediction|\n",
      "+-----+----+------+------------+---------+----+------------------+------------+-----+----+-----------+--------------------+--------------------+-------------------+\n",
      "| 2050|   5|1999.0|           3|        6|   1|          202000.0|         1.0|    2|   1|          2|[5.0,1999.0,3.0,6...|[201999.989796409...|-1448.9845224986784|\n",
      "| 2100|   2|1996.0|           1|        6|   1|151235.29235237173|         1.0|    1|   1|          2|[2.0,1996.0,1.0,6...|[151235.282167249...| -3131.806385799311|\n",
      "| 2100|   2|1999.0|           5|        4|   1| 162695.9298187809|         1.0|    1|   6|          1|[2.0,1999.0,5.0,4...|[162695.919615336...| -2593.600738449022|\n",
      "| 2100|   4|1999.0|           2|        6|   1| 162695.9298187809|         1.0|    1|   1|          2|[4.0,1999.0,2.0,6...|[162695.919616746...|  363.9405955071561|\n",
      "| 2100|   6|2002.0|           4|        6|   1|          121000.0|         1.0|    1|   1|          2|[6.0,2002.0,4.0,6...|[120999.989781346...| 3935.8751145196147|\n",
      "| 2100|   6|2002.0|          18|        6|   1|          164664.0|         1.0|    3|   1|          1|[6.0,2002.0,18.0,...|[164663.989774301...|  -886.749568335712|\n",
      "| 2100|   6|2007.0|           4|        4|   1|             501.0|         1.0|    2|   1|          2|[6.0,2007.0,4.0,4...|[500.989757052568...| 3949.9056681641378|\n",
      "| 2100|   6|2009.0|           4|        4|   1|          153280.0|         1.0|    2|   7|          6|[6.0,2009.0,4.0,4...|[153279.989743918...|  9512.205023753457|\n",
      "| 2100|   6|2010.0|           4|        6|   1|127225.02711827488|         2.0|    1|   1|          2|[6.0,2010.0,4.0,6...|[127225.016857488...|  13519.94303238066|\n",
      "| 2100|   6|2012.0|           4|        4|   1|          142446.0|         2.0|    2|   1|          3|[6.0,2012.0,4.0,4...|[142445.989728309...|  9730.549353627954|\n",
      "| 2100|   7|1998.0|           6|        6|   2|          226000.0|         2.0|    3|   5|          8|[7.0,1998.0,6.0,6...|[225999.989797012...| 1971.1791796600446|\n",
      "| 2100|   7|2003.0|           5|        6|   1|          126000.0|         1.0|    1|   1|          2|[7.0,2003.0,5.0,6...|[125999.989775224...|  5041.342765015084|\n",
      "| 2100|   7|2003.0|           9|        6|   1|           55000.0|         1.0|    1|  11|          2|[7.0,2003.0,9.0,6...|[54999.9897731525...|  9503.330361308996|\n",
      "| 2100|   8|2008.0|          16|        4|   1|          101750.0|         2.0|    1|   1|          6|[8.0,2008.0,16.0,...|[101749.989742888...| 6276.8716075550765|\n",
      "| 2100|  10|2001.0|          14|        6|   1|          198000.0|         2.0|    1|   1|          2|[10.0,2001.0,14.0...|[197999.989777611...| 1644.9670787681825|\n",
      "| 2100|  10|2003.0|           2|        6|   1|          153000.0|         1.0|    2|   1|          2|[10.0,2003.0,2.0,...|[152999.989774654...|  3641.226411320269|\n",
      "| 2100|  10|2003.0|           3|        4|   1|          198000.0|         1.0|    2|   1|          1|[10.0,2003.0,3.0,...|[197999.989773151...| -1557.869233733043|\n",
      "| 2100|  10|2003.0|           7|        6|   1| 153430.5303265941|         1.0|    1|   2|          4|[10.0,2003.0,7.0,...|[153430.520098892...|  5705.289610169828|\n",
      "| 2100|  10|2005.0|          16|        4|   1|           87000.0|         1.0|    1|   1|          2|[10.0,2005.0,16.0...|[86999.9897586718...| 1862.7338519771583|\n",
      "| 2100|  10|2006.0|           2|        6|   1|          130000.0|         2.0|    1|   1|          4|[10.0,2006.0,2.0,...|[129999.989758474...|    9364.1315024863|\n",
      "+-----+----+------+------------+---------+----+------------------+------------+-----+----+-----------+--------------------+--------------------+-------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "model = pipeline.fit(trainingData)\n",
    "prediction = model.transform(testData)\n",
    "prediction.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(price=2050, city=5, year=1999.0, manufacturer=3, cylinders=6, fuel=1, odometer=202000.0, transmission=1.0, drive=2, type=1, paint_color=2, features=DenseVector([5.0, 1999.0, 3.0, 6.0, 1.0, 202000.0, 1.0, 2.0, 1.0, 2.0]), pcaFeature=DenseVector([201999.9898, 1.1736]), prediction=-1448.9845224986784)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prediction.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Podemos observar a continuación que la columna que es mejor para la predicción de price es la de **odometer** o cuentakilómetros."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|          pcaFeature|\n",
      "+--------------------+\n",
      "|[147984.503646030...|\n",
      "|[118.989734965029...|\n",
      "|[211126.989813806...|\n",
      "|[106999.989786061...|\n",
      "|[152999.989765642...|\n",
      "|[132699.989756483...|\n",
      "|[157218.966576743...|\n",
      "|[145999.989732149...|\n",
      "|[114504.989746453...|\n",
      "|[153444.989751703...|\n",
      "|[128914.026669885...|\n",
      "|[136999.989740296...|\n",
      "|[137831.989749953...|\n",
      "|[246522.989768092...|\n",
      "|[117999.989736395...|\n",
      "|[134729.974814199...|\n",
      "|[112388.989743027...|\n",
      "|[125190.989731532...|\n",
      "|[299998.989763416...|\n",
      "|[138314.989749604...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction.select('pcaFeature').distinct().show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {
    "scrolled": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "+--------------------+\n",
      "|            features|\n",
      "+--------------------+\n",
      "|[3.0,2005.0,2.0,4...|\n",
      "|[10.0,2001.0,30.0...|\n",
      "|[23.0,2003.0,8.0,...|\n",
      "|[10.0,2004.0,7.0,...|\n",
      "|[2.0,2006.0,20.0,...|\n",
      "|[2.0,2002.0,1.0,4...|\n",
      "|[15.0,2000.0,3.0,...|\n",
      "|[24.0,2006.0,2.0,...|\n",
      "|[44.0,2006.0,12.0...|\n",
      "|[20.0,2009.0,1.0,...|\n",
      "|[2.0,2000.0,1.0,6...|\n",
      "|[2.0,2006.0,2.0,8...|\n",
      "|[9.0,2007.0,9.0,4...|\n",
      "|[1.0,2007.0,4.0,4...|\n",
      "|[19.0,2005.0,14.0...|\n",
      "|[3.0,2005.0,19.0,...|\n",
      "|[20.0,2001.0,4.0,...|\n",
      "|[10.0,2011.0,2.0,...|\n",
      "|[1.0,2007.0,30.0,...|\n",
      "|[9.0,2003.0,2.0,8...|\n",
      "+--------------------+\n",
      "only showing top 20 rows\n",
      "\n"
     ]
    }
   ],
   "source": [
    "prediction.select('features').distinct().show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![.](https://www.giitsss.com/public_assets/img/result_banner_giit.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## <span style=\"font-family:monospace;color:mediumaquamarine\">COMPARACIÓN FINAL DE LOS MÉTODOS</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RANDOM FOREST: MSE = 1.15632e+08: 💩\n",
      "RANDOM FOREST + GRID SEARCH: MSE = 1.02457e+08: 😫\n",
      "GRADIENT BOOSTED TREE REGRESSOR: MSE = 4.48182e+07: ☹️\n",
      "LINEAR REGRESSION + DISCRETIZADOS:MSE = 4.52862e-19: 🙂\n",
      "LINEAR REGRESSION: MSE = 1.51051e-19: 😀\n",
      "LINEAR REGRESSION + ESTANDARIZADOS: MSE = 2.7131e-23: 👑\n"
     ]
    }
   ],
   "source": [
    "print(\"RANDOM FOREST: MSE = %g\" % mseRF + \": 💩\") \n",
    "print(\"RANDOM FOREST + GRID SEARCH: MSE = %g\" % mseGS + \": 😫\")\n",
    "print(\"GRADIENT BOOSTED TREE REGRESSOR: MSE = %g\" % mseGBT + \": ☹️\")\n",
    "print(\"LINEAR REGRESSION + DISCRETIZADOS:MSE = %g\" % mseLRD + \": 🙂\") \n",
    "print(\"LINEAR REGRESSION: MSE = %g\" % mseLR + \": 😀\")\n",
    "print(\"LINEAR REGRESSION + ESTANDARIZADOS: MSE = %g\" % mseLRE + \": 👑\") "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![.](https://www.vinilosdecorativos.com/3998/fotomural-infantil-rayo-mcqueen-y-mate-ruta-por-el-desierto.jpg)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"font-family:monospace;color:dodgerblue\">CONCLUSIONES</span> "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "                                            ¿QUÉ HEMOS APRENDIDO?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "___"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# <span style=\"font-family:impact;font-size:3em;color:tomato\">Y</span>   <span style=\"font-family:impact;font-size:3em;color:coral\">P</span><span style=\"font-family:impact;font-size:3em;color:salmon\">A</span><span style=\"font-family:impact;font-size:3em;color:lightgreen\">R</span><span style=\"font-family:impact;font-size:3em;color:limegreen\">A</span>    <span style=\"font-family:impact;font-size:3em;color:mediumseagreen\">F</span><span style=\"font-family:impact;font-size:3em;color:turquoise\">I</span><span style=\"font-family:impact;font-size:3em;color:darkturquoise\">N</span><span style=\"font-family:impact;font-size:3em;color:deepskyblue\">A</span><span style=\"font-family:impact;font-size:3em;color:cornflowerblue\">L</span><span style=\"font-family:impact;font-size:3em;color:royalblue\">I</span><span style=\"font-family:impact;font-size:3em;color:slateblue\">Z</span><span style=\"font-family:impact;font-size:3em;color:mediumorchid\">A</span><span style=\"font-family:impact;font-size:3em;color:violet\">R</span>   <span style=\"font-family:impact;font-size:3em;color:hotpink\">...</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![.](https://www.generadormemes.com/media/created/oms18kkaw3nfr12msw8m5b3w5nbywfxcr7logoz2c7quikjn0dvx20q6azucxil.jpg)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
